# Copyright (c) 2025 Tobias Weber <weber.tobias.md@gmail.com>
#
# This file is part of the BCSV library.
#
# Licensed under the MIT License. See LICENSE file in the project root
# for full license information.

name: Benchmark Suite

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - 'include/**'
      - 'tests/**'
      - 'examples/**'
      - 'CMakeLists.txt'
      - 'tests/CMakeLists.txt'
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      type:
        description: 'Run type(s): MICRO, MACRO-SMALL, MACRO-LARGE (comma-separated)'
        required: false
        default: 'MACRO-SMALL'
      repetitions:
        description: 'Repetitions per type'
        required: false
        default: '1'

permissions:
  contents: read
  pull-requests: write

env:
  BUILD_TYPE: Release
  CXX_FLAGS: "-O3"

jobs:
  # ──────────────────────────────────────────────────────────────────
  # Quick benchmark on every PR (S size, ~30 seconds)
  # ──────────────────────────────────────────────────────────────────
  benchmark-pr:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_ENABLE_BENCHMARKS=ON \
          -DBCSV_ENABLE_MICRO_BENCHMARKS=ON

    - name: Build benchmarks
      run: cmake --build build --target bench_macro_datasets bench_micro_types bench_generate_csv -j$(nproc)

    - name: Build CLI tools
      run: cmake --build build --target csv2bcsv bcsv2csv -j$(nproc)

    - name: Run benchmark sweep (MACRO-SMALL)
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/pr-${{ github.event.number }}"
        python3 benchmark/run_benchmarks.py \
          --type=MACRO-SMALL \
          --repetitions=1 \
          --build-type=Release \
          --git=pr-${{ github.event.number }} \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/pr-${{ github.event.number }}"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "RUN_ROOT=$RUN_ROOT" >> "$GITHUB_ENV"
        echo "RUN_DIR=$RUN_DIR" >> "$GITHUB_ENV"

    - name: Verify report exists
      run: |
        test -f "$RUN_DIR/report.md"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-pr-${{ github.event.number }}
        path: |
          ${{ env.RUN_DIR }}
        retention-days: 30

    - name: Post summary to PR
      if: always()
      run: |
        REPORT="$RUN_DIR/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark Results (MACRO-SMALL — PR #${{ github.event.number }})" > comment.md
          echo "" >> comment.md
          cat "$REPORT" >> comment.md
          cat comment.md >> "$GITHUB_STEP_SUMMARY"
        else
          echo "⚠️ Benchmark report not generated" >> "$GITHUB_STEP_SUMMARY"
        fi

  # ──────────────────────────────────────────────────────────────────
  # Full benchmark on release tags (L size, ~2 minutes)
  # ──────────────────────────────────────────────────────────────────
  benchmark-release:
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout release
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_ENABLE_BENCHMARKS=ON \
          -DBCSV_ENABLE_MICRO_BENCHMARKS=ON \
          -DBCSV_ENABLE_EXTERNAL_CSV_BENCH=ON

    - name: Build all benchmarks
      run: cmake --build build -j$(nproc)

    - name: Run full benchmark sweep
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/release-${{ github.ref_name }}"
        python3 benchmark/run_benchmarks.py \
          --type=MICRO,MACRO-SMALL,MACRO-LARGE \
          --repetitions=3 \
          --build-type=Release \
          --git=${{ github.ref_name }} \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/release-${{ github.ref_name }}"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "RUN_ROOT=$RUN_ROOT" >> "$GITHUB_ENV"
        echo "RUN_DIR=$RUN_DIR" >> "$GITHUB_ENV"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.ref_name }}
        path: |
          ${{ env.RUN_DIR }}
        retention-days: 90

    - name: Post release summary
      run: |
        REPORT="$RUN_DIR/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark Results — ${{ github.ref_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          cat "$REPORT" >> "$GITHUB_STEP_SUMMARY"
        fi

  # ──────────────────────────────────────────────────────────────────
  # Manual dispatch — configurable size and mode
  # ──────────────────────────────────────────────────────────────────
  benchmark-manual:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_ENABLE_BENCHMARKS=ON \
          -DBCSV_ENABLE_MICRO_BENCHMARKS=ON \
          -DBCSV_ENABLE_EXTERNAL_CSV_BENCH=ON

    - name: Build all benchmarks
      run: cmake --build build -j$(nproc)

    - name: Run benchmarks (${{ inputs.type }})
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/manual"
        python3 benchmark/run_benchmarks.py \
          --type=${{ inputs.type }} \
          --repetitions=${{ inputs.repetitions }} \
          --build-type=Release \
          --git=manual \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/manual"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "RUN_ROOT=$RUN_ROOT" >> "$GITHUB_ENV"
        echo "RUN_DIR=$RUN_DIR" >> "$GITHUB_ENV"

    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-manual
        path: |
          ${{ env.RUN_DIR }}
        retention-days: 90

    - name: Post summary
      run: |
        REPORT="$RUN_DIR/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark (${{ inputs.type }})" >> "$GITHUB_STEP_SUMMARY"
          cat "$REPORT" >> "$GITHUB_STEP_SUMMARY"
        fi
