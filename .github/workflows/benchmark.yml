# Copyright (c) 2025 Tobias Weber <weber.tobias.md@gmail.com>
#
# This file is part of the BCSV library.
#
# Licensed under the MIT License. See LICENSE file in the project root
# for full license information.

name: Benchmark Suite

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - 'include/**'
      - 'tests/**'
      - 'benchmark/**'
      - 'examples/**'
      - 'CMakeLists.txt'
      - 'tests/CMakeLists.txt'
      - 'benchmark/CMakeLists.txt'
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      type:
        description: 'Run type(s): MICRO, MACRO-SMALL, MACRO-LARGE (comma-separated)'
        required: false
        default: 'MACRO-SMALL'
      repetitions:
        description: 'Repetitions per type'
        required: false
        default: '1'

permissions:
  contents: read
  pull-requests: write

env:
  BUILD_TYPE: Release
  CXX_FLAGS: "-O3"

jobs:
  # ──────────────────────────────────────────────────────────────────
  # Quick benchmark on every PR (S size, ~30 seconds)
  # ──────────────────────────────────────────────────────────────────
  benchmark-pr:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_BUILD_BENCHMARKS=ON \
          -DBCSV_BUILD_MICRO_BENCHMARKS=ON

    - name: Build benchmarks
      run: cmake --build build --target bench_macro_datasets bench_micro_types bench_generate_csv -j$(nproc)

    - name: Build CLI tools
      run: cmake --build build --target csv2bcsv bcsv2csv -j$(nproc)

    - name: Build core tests
      run: cmake --build build --target bcsv_gtest test_c_api test_row_api -j$(nproc)

    - name: Run core tests
      run: ctest --test-dir build --output-on-failure

    - name: Run benchmark sweep (MACRO-SMALL)
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/pr-${{ github.event.number }}"
        python3 benchmark/run.py wip \
          --type=MACRO-SMALL \
          --repetitions=1 \
          --build-type=Release \
          --git=pr-${{ github.event.number }} \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      id: resolve_pr
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/pr-${{ github.event.number }}"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "run_dir=$RUN_DIR" >> "$GITHUB_OUTPUT"

    - name: Verify report exists
      run: |
        test -f "${{ steps.resolve_pr.outputs.run_dir }}/report.md"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-pr-${{ github.event.number }}
        path: |
          ${{ steps.resolve_pr.outputs.run_dir }}
        retention-days: 30

    - name: Post summary to PR
      if: always()
      run: |
        REPORT="${{ steps.resolve_pr.outputs.run_dir }}/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark Results (MACRO-SMALL — PR #${{ github.event.number }})" > comment.md
          echo "" >> comment.md
          cat "$REPORT" >> comment.md
          cat comment.md >> "$GITHUB_STEP_SUMMARY"
        else
          echo "⚠️ Benchmark report not generated" >> "$GITHUB_STEP_SUMMARY"
        fi

  # ──────────────────────────────────────────────────────────────────
  # Full benchmark on release tags (L size, ~2 minutes)
  # ──────────────────────────────────────────────────────────────────
  benchmark-release:
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout release
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_BUILD_BENCHMARKS=ON \
          -DBCSV_BUILD_MICRO_BENCHMARKS=ON \
          -DBCSV_ENABLE_EXTERNAL_CSV_BENCH=ON

    - name: Build all benchmarks
      run: cmake --build build -j$(nproc)

    - name: Run core tests
      run: ctest --test-dir build --output-on-failure

    - name: Run full benchmark sweep
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/release-${{ github.ref_name }}"
        python3 benchmark/run.py wip \
          --type=MICRO,MACRO-SMALL,MACRO-LARGE \
          --repetitions=3 \
          --build-type=Release \
          --git=${{ github.ref_name }} \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      id: resolve_release
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/release-${{ github.ref_name }}"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "run_dir=$RUN_DIR" >> "$GITHUB_OUTPUT"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.ref_name }}
        path: |
          ${{ steps.resolve_release.outputs.run_dir }}
        retention-days: 90

    - name: Post release summary
      run: |
        REPORT="${{ steps.resolve_release.outputs.run_dir }}/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark Results — ${{ github.ref_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          cat "$REPORT" >> "$GITHUB_STEP_SUMMARY"
        fi

  # ──────────────────────────────────────────────────────────────────
  # Manual dispatch — configurable size and mode
  # ──────────────────────────────────────────────────────────────────
  benchmark-manual:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y -q ninja-build python3-matplotlib

    - name: Configure CMake (Release)
      run: |
        cmake -S . -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ env.BUILD_TYPE }} \
          -DCMAKE_CXX_FLAGS="${{ env.CXX_FLAGS }}" \
          -DBCSV_BUILD_BENCHMARKS=ON \
          -DBCSV_BUILD_MICRO_BENCHMARKS=ON \
          -DBCSV_ENABLE_EXTERNAL_CSV_BENCH=ON

    - name: Build all benchmarks
      run: cmake --build build -j$(nproc)

    - name: Run core tests
      run: ctest --test-dir build --output-on-failure

    - name: Run benchmarks (${{ inputs.type }})
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/manual"
        python3 benchmark/run.py wip \
          --type=${{ inputs.type }} \
          --repetitions=${{ inputs.repetitions }} \
          --build-type=Release \
          --git=manual \
          --results="$RUN_ROOT" \
          --no-build

    - name: Resolve run directory
      id: resolve_manual
      run: |
        RUN_ROOT="${{ runner.temp }}/bench_results/manual"
        RUN_DIR=$(ls -1dt "$RUN_ROOT"/* | head -1)
        echo "run_dir=$RUN_DIR" >> "$GITHUB_OUTPUT"

    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-manual
        path: |
          ${{ steps.resolve_manual.outputs.run_dir }}
        retention-days: 90

    - name: Post summary
      run: |
        REPORT="${{ steps.resolve_manual.outputs.run_dir }}/report.md"
        if [ -f "$REPORT" ]; then
          echo "## Benchmark (${{ inputs.type }})" >> "$GITHUB_STEP_SUMMARY"
          cat "$REPORT" >> "$GITHUB_STEP_SUMMARY"
        fi
