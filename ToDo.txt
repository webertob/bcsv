Decisions: The file format uses 8-bit/1-byte alignment. Higher-order alignment is not considered, as we favor small file size. For temporary data and algorithms, higher-order alignment should be used.

0.  Write project mission statement, upload to git hub
    Aims on combining the easy usage of CSV files with the performance and file size of a binary format. It should perform specifically well for large time series data sets and also cover small embedded platforms. Therefore it is an explicit requirement that we support files that are larger than the available memory (RAM), thus the file is read in chunks or rows. For the same reason the computational effort should be reasonable small. In order to support real-time processing the call time to write or read data should be constant. 
    The file should exploit time-series data structure to reduce file size. Specifically it should be efficient on recording  Binary Wave Forms and constant data (spars recording / events) to achieve good compression. Compression ratio to be balanced with requirements on computational and streaming. It should be possible to read data from the file even if write was interrupted. We want to be able to retrieve the last fully written row. The file format is to be optimized for sequential row wise read and write operations, but should support random read with acceptable speeds, too. 
    The file should feel natural to programmers and compete with CSV with respect of ease of usage. Therefore, we avoid definition or validation of a schema. The file should be defined from within the programming language itself (C/C++, python, C# or …), without the need to run another tool or configuration file format. This is different to FlatBuffers or ProtoBuff. The file structure itself is documented in the file, similar to a header in a CSV. The header in BCSV is mandatory, as we won’t waste storage for delimiters. BCSV also enforces the type defined in the header for each column on every row. Thus you only need to validate the file structure once when you open a file and can rely on every row to show up with a valid type in each column (cell). 

    Design Goals:
    -	Sequential recording of a 1000 channel stream with 1KHz on an STM32F4 and 10KHz on a STM32F7, Zynq7000, RaspPi-nano
    -	Small file growth < 1KB/s for 1000 channel stream with 10Khz if nothing happens, except counting a clock/counter. 
    -	Processing >= 1 million rows/sec for a 1000 channel stream on a modern Zen3 CPU core.
    -	<30% file size compared to CSV comparing a 1000 channel stream
    -	Compatible with C/C++, C# and Python

    A set if CLI tools should help working with the files, supporting common work flows (view files, filter files, split and merge files, convert from and other formats).
    Who is the customer: Anybody how is running metrology tasks with digital tools. 


0. Test and Stabilize direct access feature
    - build and run unit tests
    - build and run benchmarks
1. Test and Stabilize CLI tools:
    a. bcsv2csv
    b. csv2bcsv
    c. bcsvHead
    d. bcsvTail
2. Benchmark BCSV 1.3 against 1.2
3. Test and Stabilize C_API -> C#
4. Test and Stabilize Python

5. Refactoring:
    - size_t vle_encode(value, void* dst, size_t dst_capacity)
    - void   vle_encode(value, ByteBuffer &bufferToAppend)
    - size_t vle_decode(&value, void* src, size_t src_capacity) //returns the number of bytes consumed
    - value  vle_decode(std::span<std::byte> &bufferToRead)     //span gets updated, removing the bytes consumed
    - bool   vle_read(&value, &istream, *hash = 0)
    - bool   vle_write(&value, &ostream, *hash = 0)

    - size_t ble_encode(value, void* dst, size_t dst_capacity)
    - void   ble_encode(value, ByteBuffer &bufferToAppend)
    - size_t ble_decode(&value, void* src, size_t src_capacity) //returns the number of bytes consumed
    - value  ble_decode(std::span<std::byte> &bufferToRead)     //span gets updated, removing the bytes consumed
    - bool   ble_read(&value, &istream, *hash = 0)
    - bool   ble_write(&value, &ostream, *hash = 0)

    - Modify Row to not use std::variants, consider alignment

6. Implement new row encoding schema.
7. Implement File Flags (raw, stats)
8. Implement Column Modifiers (index, volatile, monotonic, ordered)


3.Variable length encoding (float and integer values)
    Main usage of bcsv is the handling of time-series data. Thus we expect many rcurring or constantly falling or raising values and values that are similar to previouse values. In many cases the full extend of a variable type is not required to encode the actual value.
    This feature is working on a per row level, not only a single cell/value as the current vle_encode / vle_decode functions do. It tries to provide a better balance between computational effort and compression and takes advantange on previouse data (time-series).
    
    Design goals:
    1. shortest possible encoding for: 
        - const values (Zero-Order-Hold), row(n).col(i) = row(n-1).col(i)
        - monoton raising/falling values (first-order), row(n).col(i) = row(n-1).col(i) + (row(n-1).col(i) - row(n-2).col(i))
        - bool to be stored directly as single bit
    2. small overhead in worst case scenarios
    3. exploit variable length to shorten storage for large types
    4. exploit delta encoding to shorten storage for large types
    5. allow easy comparision between two rows to identify equality in typical scenarios (no change, linear change)

    Not finaly decided yet:
        - store column encoding data in a dedictate area (row header) or close to data? Current pitch: lets implement a row header that does not require to follow byte alignment rules 
            - Bit-0 x N
            - Bis 1:2 x N (-bools, -rep)
            
        - 8bit aligned / byte aligned data?
        
    Bit-0 Repetion (Rep)
        0: check bits 1:5 for additional information on column encoding
        1: column encoding is same as previouse row bit3 1:5 are skipped
        Note: 
            1. For column type=bool, this column directly holds the value, no further bits required!
            2. For column hint=volatile, this column directly switches const/plain.
    Bit 1:2: column encoding
        00: const, previouse value is kept constant, bits 3:5 are skipped
        01: plain, bits 3:5 contain length of field, bits 6:14 contain data, data to simply contain the value we search
        10: extra, extrapolate using diff between row(n) = row(n-1) + row(n-1) - row(n-2), bits3:5 (length) and bits 6:14 (data) are skipped
        11: delta, delta relative to previouse row row(n) = row(n-1) + delta;
        Note: This filed is not present for variables maked volatile!
    Bit 3:5 length of the following data field, only present if data is provided and length is undefined, length is defined as number of bytes!
        for 1 byte types this field has a length of 0 bits (as data filed implictily is defined to length of 1 byte)
        for 2 byte types this field has a length of 1 bit  (0:short (1byte), 1:long (2bytes))
        for 4 byte types the filed has a maximum length of 2 bits (00:1byte, 01:2bytes, 10:3bytes, 11:4bytes)
        for 8 byte types the filed has a maximum length of 3 bits (double, unit64) it has a length of 
    Bit 6:14 data actual data stored for this variable

    Special cases: For small types (1 byte) avoid unnecessary overhead (complexity, runtime, control bits) that won't pay off
5. dictionary encoding for strings
    For strings a low cardinality is expected (many repetitions of the same string) are expected. Thus a dictionary based compression should be implemented. Exploit similar approach as for numerical values.
    Store string at its first occurens and assign ID (simply increment number, within the current packet)
    Other occurences just to reference that number

6. implement additional FileFlags
    raw: omit all the complex variable length encoding to save compute
    stats: each packet to contain some stats on what was included in each column, !min!, !max!, mean, median, std
    stream: omit packet structue and fileindex. also to save compute on constraint architecture, expected to come with significant performance constrains for random read. -> convert to Normal (packet bases) file structure in that case.


7. implement Column modifier:
    - volatile: this parameter is changing arbitrarily, don’t spend too much effort to compress it. 
    - index: there is defined relationship between this value and the row number, i.e. a timestamp or counter.
    - monotonic: defined to grow with a nearly constant rate
    - unique: we guarantee there is no duplication of entries in this column 
    - ascending: this value can only grow for subsequent rows.
    - descending: this value can only fall for subsequent rows.

8. Create random access / random read methods:
    Row row(size_t index) {
        if(index == current_index+1) {
            //fall back to sequential
            readNext();
            return row_;
        }
        //check if row is in current packet, if not read packet required, build an file wide index if needed
        // use specific heuristics for ZoH, i.e. traverse several rows back and for to avoid complex reads
    }
    const auto& cell(size_t rowIndex, size_t colIndex) // To directly access a cell, use RowView as a lightweight abstraction to avoid full deserialization

9. Create sampler function / API to select data based on certain conditions: 
    - Support C++ interface (iterate over individual rows)
    - Support Python interface (iterate over individual rows and block reads (pandas/numpy))

10. CLI tools:
    - Modify CLI tools to utilize random read functionality to avoid parsing the entire file (when possible):
        - bcsvTail
        - bcsvHead
        - bcsv2csv in combiniation with slicing
    - create: bcsv2Parquet
    - create: parquet2bcsv
    - create: bcsvInspect (checks if the file is complet or damaged (i.e. index missing), can repair file/index, prints information on the file content)
    - create: bcsvSampler (creates an down sampled version, using specific rules provided)
    - create: bcsvCompress (insepects the file and tries to apply a more agressive compression, using 2-phase inspection and a file wide dictionary)

11. Create a concept to maintain backward compatibility atleast for reading/translating files!
Out of Scope (for now):
13. ROS integration (subscribe ROS topics and store to bcsv and reverse)
14. MQTT integration (read MQTT topics and store to bcsv and reverse)
15. Consider performance improvements by enforcing alignment for data types used during read and write, to enable more agressive compiler optimization (alignas())
16. Dynamic BitSet, ensure wide types (uin64/32/16 are used if possible), also consider SIMD/AVX
17. Add SIMD / vectorized interfaces:
    - Python: implement get/set functions for numpy
    - C#/Unity: compatibility with componet/job system

