background:
-  we are developing the bcsv library. The Binary-CSV (BCSV) library that tries to combine the flexibility of CSV-files with the speed and size efficency of a binary format. See README.md and ARCHITECTURE.md ffor background.
- bcsv sources are located in ./include/bcsv
- examples demonstrating usage of the BCSV, also used for debugging and quality assurance are located in ./examples
- test are located in ./tests
- ./tmp provides space for temporary experiments and runs and is not version controlled (ignored) by git
- we are using cmake & ninja for builds
- we are using google test for tests ./tests/bcsv_gtest
- benchmark suite is located under:
- documentation (also on the API) is provided here:



[x] 0.  Write project mission statement, upload to git hub
    Aims on combining the easy usage of CSV files with the performance and file size of a binary format. It should perform specifically well for large time series data sets and also cover small embedded platforms. Therefore it is an explicit requirement that we support files that are larger than the available memory (RAM), thus the file is read in chunks or rows. For the same reason the computational effort should be reasonable small. In order to support real-time processing the call time to write or read data should be constant. 
    The file should exploit time-series data structure to reduce file size. Specifically it should be efficient on recording  Binary Wave Forms and constant data (spars recording / events) to achieve good compression. Compression ratio to be balanced with requirements on computational and streaming. It should be possible to read data from the file even if write was interrupted. We want to be able to retrieve the last fully written row. The file format is to be optimized for sequential row wise read and write operations, but should support random read with acceptable speeds, too. 
    The file should feel natural to programmers and compete with CSV with respect of ease of usage. Therefore, we avoid definition or validation of a schema. The file should be defined from within the programming language itself (C/C++, python, C# or …), without the need to run another tool or configuration file format. This is different to FlatBuffers or ProtoBuff. The file structure itself is documented in the file, similar to a header in a CSV. The header in BCSV is mandatory, as we won’t waste storage for delimiters. BCSV also enforces the type defined in the header for each column on every row. Thus you only need to validate the file structure once when you open a file and can rely on every row to show up with a valid type in each column (cell). 

    Design Goals:
    -	Sequential recording of a 1000 channel stream with 1KHz on an STM32F4 and 10KHz on a STM32F7, Zynq7000, RaspPi-nano
    -	Small file growth < 1KB/s for 1000 channel stream with 10Khz if nothing happens, except counting a clock/counter. 
    -	Processing >= 1 million rows/sec for a 1000 channel stream on a modern Zen3 CPU core.
    -	<30% file size compared to CSV comparing a 1000 channel stream
    -	Compatible with C/C++, C# and Python

    A set if CLI tools should help working with the files, supporting common work flows (view files, filter files, split and merge files, convert from and other formats).
    Who is the customer: Anybody how is running metrology tasks with digital tools. 


[x] 0. Test and Stabilize direct access feature
    - build and run unit tests  (passed)
    - build and run benchmarks  (passed)
[x] 1. Test and Stabilize CLI tools:
    a. bcsv2csv
    b. csv2bcsv
    c. bcsvHead
    d. bcsvTail
[x] 2. Test and Stabilize C_API -> C#
[x] 3. Test and Stabilize Python

[x] 4. Refactor VLE (Variabel Length Encoding):
    Note:
        We won't distinguish between VLE and BLE as we only use "block length encoding" to implement variable length encoding. This means we are string the number of additioaly required bytes required in an block of consecutive bits at the very begining of the byte stream considered.
        Use templates and expolit knowlegde about the type to improve performance.
            - fallthrough for single byte types
            - zigzag encoding only for signed types
            - destinguish between truncated and normal length mode (truncated means the bits required to encode the length are taken from the actual value, e.g. for uint64_t truncated only leaves 61bit for value)
            - we encode the number of ADDITIONAL bytes required to encode/decode the value, additional to the 1st byte that must be present anyway e.g. for uint64_t the range is 0 to 7)
            - avoid loading or shifiting indivudal bytes and replace this with opertions on larger registers if possible to achive higher performance, consider speculative operations and masking (e.g. always load 8bytes for an uint64) if performance can be gained without corrupting the I/O. We now from experiments that this can be highly profitable, espacially when used on data already in memory, but that travesing back and forth in a std::stream has a significant performance penaltiy and must be avoided. 

    Functions required:
    - size_t vle_encode(const T &value, void* dst, size_t dst_capacity)  //returns the number of bytes written to dst to store the value
    - void   vle_encode(const T &value, ByteBuffer &bufferToAppend)      //appends the bytes to the ByteBuffer increasing its size
    - size_t vle_decode(T &value, void* src, size_t src_capacity) //returns the number of bytes consumed
    - T      vle_decode(std::span<std::byte> &bufferToRead)     //span gets updated, removing the bytes consumed
    - bool   vle_read(T &value, &istream, *hash = 0)
    - bool   vle_write(const T &value, &ostream, *hash = 0)

[x] 5. Consider a robust handling of ColumnNames. 
    - do not modify ColumnNames protected by ""
    - allow dupplicate ColumnNames
   
[x] 6. Modify Row and Layout to not use std::variants, consider alignment
    [x] get it compiled
    [x] pass tests
    [x] pass benchmark
    [o] update tools --> [10]
    [o] commit
    [0] build and test python --> [10]
    [o] build and test C# --> [10]

[x] 7. Refactor bitset_dynamic to use full register width instead of narrow bytes. Thus we want to use 64bit types on 64bit platform. Consider alignment.

[x] 8. Add visit() function to Row, RowStatic, RowView, RowViewStatic enable fast operations on the data 

[x] 8.b: Exploit visit function to simplify code [EVALUATED - NO ACTION NEEDED]
        Analysis completed 2026-02-07. See Implementation_Visit()_Function.MD for details.
        Conclusions:
        - visit on single column? 
          ❌ NOT RECOMMENDED: Current get<T>() is more efficient and clearer
        - visit on column range? 
          ⏸️ DEFER TO PHASE 2+: Useful feature but wait for user demand
          Simple to add later: visit(start_col, end_col, visitor)
        - Implement get/set via visit()? 
          ❌ NOT RECOMMENDED: Performance & clarity > DRY principle
          Switch/case duplication is acceptable for hot-path operations
        
        Decision: Keep current implementation. The switch/case in get(), set(), 
        and visit() serves different performance/API requirements. No changes needed.

[x] 8.c: Refeactoring Row, Layout relation ship. 
    Currently Layout is member (owned by) Row. However we typically have multiple rows that share the same Layout. 
    This leads to a waste of memory, but also enforces us to check if layouts match between rows, hence slowing down operation between rows, which we most likley need when we do more complex delta encoding.
    Therefore I would prefere to have row just having a pointer to Layout. Such it would be easy to check if two rows share the same layout and reduces the amount of memory required (hot-cache).
    However we need to ensure that Layout does not change, as long as rows are alive or that they follow relevant changes to the layout structure i.e. setColumnType, addColumn, removeColumn. Name changes are irrelevant.
    I still like the current API of the BCSV library and would like to maintain the workflow, see examples and benchmark.
[x] 8.d: Finalize Row::onAddColumn, Row::onChangeColumnType, Row::onRemoveColumn
[x] 8.e: debug large column count bools
[x] 8.f: Add test for bitset insert
[x] 8.g: Improve bcsv::bitset:
        Optimize memory allocation and layout for dynamic bcsv::bitset<> using (Small Object Optimization). 
        Goal: avoid heap allocations and improve cache locality in the most commone use cases of having small bit counts.
        Requirments: maintain current API and performance level. Try to avoid significant code growth, try to maintain commonality between static and dynamic bitsets bcsv::bitset<N> and bcsv::bitset<>
        Concept: 
            Make bcsv::bitset<> to have exactly 2 member variables, which are defined as: size_t size_; and uintptr_t data_;
            size_: is going to hold the number of bits (N) currently managed by this bitset. Therefore: We don't store capacity. We don't store number of words. These need to be calculated on the fly as required. See notes.
            data_: content of data_ deppends on size_. 
                    If N is small enough to be stored within one word (N<=sizeof(uintptr_t)*8):
                        bits are stored directly within data_ on the stack. 
                    else if N grows bejond the capacity of a single word. 
                        We need to allocate the required amount of words on the heap and data_ holds a pointer to that memory.

            Note: implement a private function bitset::wordCount()const; That returns the number of words available within data_. it ca be easily calculatey on the fly from bitset::size_ e.g. divide by 64 on 64bit platforms (shift by 6) or devide by 32 on 32bit platforms (shift by 5). No need to make it a persitent member of bitset<>. 
                implement a public function bitset::capacity(); We expect the layout of bitset to be quite stable. Thus we are not tracking capacity as an independent member. Therefore when a reduction of N causes a reduction of word count, we need to reallocate and move to a smaller chunk of memory to not lose track of the upper part of memory. 
                the return of bitset.capacity() can only be slighly larger than bitset.size(), e.g: return capacity = wordCount() * sizeof(word) * 8;
[x] 8.h Make change tracking for Row and RowStatic a compile time decision.
            Issue: currently we have a lot of branching depending on change tracking beeing enabled or disabled. But in normal operations, this decision only needs to be done once. For instence when the User decides to Write using ZoH, this basically requiers change tracking enabled.
            Goal: With minimum impact to the API make changeTracking a compile time decision. Propably using a template parameter
[x] 8.h: Restructure Row object to not use ptr_. 
            Delivered: 
                - Three-container storage: bits_ (bitset<>), data_ (vector<byte>), strg_ (vector<string>). Removed ptr_.
                - Moved offsets_ to Layout::Data. Added Layout::Data::computeOffsets() / rebuildOffsets().
                - Added bool_mask_ and tracked_mask_ (bitset<>) to Layout::Data for O(1) mask-based operations.
                - Incremental addColumn optimization: O(1) for BOOL append, O(n) for STRING/scalar or mid-insert. Avoids full rebuildOffsets().
                - Sequential serializeToZoH: writes bits_ directly to buffer (memcpy), early exit if no tracked changes.
                - get<T>/ref<T>() return by value. Removed bool_cache_. Supports string_view and span<const char> for zero-copy reads.
                - Removed ref<T>() const (was duplicate of get<T>). Kept mutable ref<T>() for both Row and RowStatic.
                - setChanges/resetChanges use bitwise mask ops (|= trackedMask, &= boolMask) instead of loops.
                - hasAnyChanges() uses bits_.any() — intentionally includes bool values (any true bit = data to serialize).
                - Fixed thread_local bool aliasing in raw get(size_t): alternating 2-slot buffer.
            Note: Raw get(size_t) returning const void* is a legacy API, not called externally. Pointer to bool
                  columns is only valid until the next get(size_t) call for a bool column on the same thread.
            Tested: 299/299 tests pass (debug + release). All examples and benchmarks verified.

[x] 9. Improve benchmark suite — see benchmark/BENCHMARK_PLAN.md for authoritative plan
    [x] Phase 1 — MVP: Fair CSV + Key Datasets + JSON Output + Basic Report
        - fair CSV baseline using Row::visitConst() + std::to_chars() (no ostream overhead)  
        - CsvReader using std::from_chars() for all types including float/double
        - 4 dataset profiles: mixed_generic, sparse_events, sensor_noisy, string_heavy
        - bench_macro_datasets: CSV / BCSV Flexible / BCSV ZoH, round-trip validation, JSON output
        - bench_micro_types: Google Benchmark per-type Get/Set/Visit/Serialize
        - Python orchestrator (benchmark/run_benchmarks.py) + report generator
        - Files: tests/bench_common.hpp, bench_datasets.hpp, bench_macro_datasets.cpp, bench_micro_types.cpp
    [x] Phase 2 — Full Dataset Coverage + CLI Tool Benchmarks + Persistence
        - add 5 dataset profiles: bool_heavy, arithmetic_wide, simulation_smooth, weather_timeseries, high_cardinality_string
        - add --size=S|M|L|XL flag (10K/100K/500K/2M rows)
        - add --benchmark flag to bcsv2csv and csv2bcsv CLI tools
        - integrate CLI tool benchmarks into orchestrator (csv→bcsv→csv round-trip)
        - persistent result storage in benchmark/runs/<hostname>/<timestamp>/, cleanup temp files
    [x] Phase 3 — External Comparison, Reporting, Regression Detection
        - optional external CSV lib benchmark (vincentlaucsb/csv-parser via FetchContent)
        - enhanced Python reporting: bar charts, speedup tables, compression ratios
        - benchmark/compare_runs.py: regression detection, persistent leaderboard
    [x] Phase 4 — Polish, CI Integration, Documentation
        - GitHub Actions workflow: sweep on PR, full on release
        - update README.md and tests/README.md with benchmark docs
        - deprecate legacy benchmark_large.cpp and benchmark_performance.cpp
    Follow-ups (from Phase 2 analysis):
        [ ] Multi-repetition support: add --repeat=N flag to bench_macro_datasets with mean ± stddev reporting
        [x] CPU pinning: taskset -c 0 wrapper in run_benchmarks.py (--no-pin to disable)
        [x] Warm-up run: 100-row warmup iteration runs automatically before measurement

[x] 9.b Do improve efficency on AI Co-development create persitent information about the project, supports AI-agents to quickly onboard and understand the goals, codes and infrastrutcure.
    - build skill
    - test skill
    - benchmark skill
    - project goals
    - project constrains
    - code formation rules & preferences
    - project structure / file structure
    DONE — Created:
        SKILLS.md (root)        — master AI onboarding doc (build, test, API, naming, priming prompt)
        tests/SKILLS.md         — test build/run/sanitizer commands, test file inventory
        examples/SKILLS.md      — example targets, CLI tools quick reference
        python/SKILLS.md        — Python build/install/test, header sync, API
        unity/SKILLS.md         — Unity/C# architecture, P/Invoke, memory management
        .clang-format           — project formatting rules (LLVM base, indent 4, braces Attach)
    Naming convention enforcement (snake_case_ private members, snake_case public struct members, lowerCamelCase functions, UpperCamelCase classes, CAPITAL_SNAKE_CASE constants):
        Wave 1: definitions.h (ALWAYS_FALSE, GetTypeT), vle.hpp (all functions+constants), bitset (class Bitset, DYNAMIC_EXTENT, Reference, SliceView)
        Wave 2: binary struct members (FileHeader/PacketHeader/FileFooter ConstSection, PacketIndexEntry)
        Wave 3: layout.h (ColumnDefinition, Change, Callbacks members; LayoutStatic COLUMN_TYPES)
        Wave 4: row.h (COLUMN_COUNT, COLUMN_LENGTHS, COLUMN_OFFSETS, OFFSET_VAR)
        Wave 5: reader/writer members (~15 camelCase_ → snake_case_), constSection_ → const_section_, is_open() → isOpen()
        Wave 6: C API bcsv_layout_isCompatible → bcsv_layout_is_compatible
    Public struct member refinement: removed trailing underscores from public struct members (snake_case, not snake_case_).
        Affected: FileHeader::ConstSection (8), PacketHeader (3), FileFooter::ConstSection (4), PacketIndexEntry (2), ColumnDefinition (2), Change (3), Callbacks (1).
        Private members (const_section_, packet_index_, callbacks_, etc.) correctly retain trailing underscore.
        Updated SKILLS.md naming table to distinguish public vs private member convention.
    All 327 GTest + 76 C API + Row API tests pass on both debug and release builds.

10. Change "flat" (none-ZoH) wire format to optimize compression and read & write performance
        Porpoposal new format should look like this: 
            - bits_ (directly write Bitset to buffer, using Bitset.writeTo() and Bitset.readFrom() functions. Aligns to single byte boundary.
            - data_ (all primivites / arithmetics, but remove padding, no alignment)
            - strg_lengths [vector(sequenc) of uint16_t] (as many entries as string columns defined in layout
            - strg_data [vector of const char*, strings payload]
        - faster serializatzion / deserialization as wire format is closer to Row format, no need to calculate strAddr.
        - small footprint (bool to be stored as bits), no strAddr no offsets stored and calculated


11. Restructure Interaction between Row(classes) and Reader/Writer. Proposal: Introduce a Serializer / Deserializier layer within the API that is going to allow us to support different Encoding schemas, limiting the impact on Row, Reader and Writer classes.
        Issue: Currently serializer and deserailizer functionalities are tightly integrated with row, this:
            - makes row heavier (memory) and more complex than required, as it stores data required for serilization/deserialization, e.g. (wire)offsets_ within RowView and offset_var_ in all Row classes.
              which in many use cases may not be needed and limits rows ability to be used as lightweight facility to collect and transfer information within an application.
            - duplication and distribution of offset_ and offset_var_ over multiple Row instances, which reduces cache-locality and increases cache-misses and potentially causing a performance penaltiy
            - For upcoming delta encoding schedmal, we are going to have multiple rows active during serialization/deserialization, as we need to calculate deltas between them or rebuild from deltas.
            - For ZoH or upcoming more complex delta encoding, a persitent context between subsequent rows is required. Currently Writer/Reader provideds this context.
            - However as we intend to support different Serialization/Deserialization schemas, to match specific application environments (embedded, sequential, radnom-access, high through-put, high compression). Reader&Writer might grow in complexity quickly.
            - Reader & Writer are API relevant interaction points, that should be kept stable, there fore Serilaization/Deserialization 
            - Row implementation grows complex --> currently ~3500 lines of code in row.hpp, we should split for maintain ability. --> Take serialization/deserilization out.
            - In order to provide backward compatibility to support older file formats, Reader/Writter should be able to switch the serialization/deserialiozation engine
        Proposal: Implement a Serializer and Deserailizier layer providing a stable interface towards Reader/Writer and Row(s) to enable different encoding schemas (i.e. flat (ToDo 12.), ZoH, delta (ToDo 13.)), with little to now impact on Row(s), Writers and Readers.
            Ideas:
                A. Serializer/Deserializer to operate on uncompressed data. Thus, it is callers job to manage compression and packkage structure. --> Still a considerable amount of complexity remains within Reader/Writer
                B. Serializer/Deserializer operate on a single row (per call). Thus, it is callers job to manage packets. 
                    Does it make sense to seperate:
                        RowSerializers/Deserializers for encoding/decoding rows from/to uncompressed data, can have knowledge about multiple rows (delta encoding)
                        FileSerializers, manages header, packet structure and compression.
        Goals:
        Constrains:
        Tasks:
            proposal:
                Serializer<Layout, Row> {
                    bool do(const Row& row, ByteBuffer &buffer);
                    setup?
                    reset?
                    stats?
                }
                Deserializer<Layout, Row> {
                    bool do(Row& row, std::span<std::byte> buffer);
                    setup?
                    reset?
                    stats?
                }
        - Create Serilizer/Deserializer for Flat and ZoH encoding --> dense columns (Row, RowStatic)
        - Create Serilizer/Deserializer for Flat and ZoH encoding --> sparse columns (RowView, RowViewStatic)
        - Transfer Row, RowView, RowStatic, RowViewStatic serializer and deserializer functions to the new Classes
        - Modify Reader/Writer to use the new serializer
        - Check that we have maintained current bcsv API
            - run&pass tests
            - run&pass benchmarks
            - build examples and CLI tools

12. Implement CSV write and read functionality using the BCSV interface
    10.a: Restructure BCSV Reader/ Writer API
        - Create a ReadAPI and WriterAPI class that defined the basic functionality each Reader and Writer needs to support.
        - Prepare for spars rows (RowView and direct access)
            Reader<Layout, Row> --> Reader<Layout, RowView>
            - row()
        - Make it easier to write your own Reader/Writer, while using the BCSV ecosystem:
            - Layout and Row interface --> continue using the CLI tools
    10.b: Develop LayoutCSV, ReaderCSV and WriterCSV
        - add tests csv-to-csv, csv-to-bcsv, bcsv-to-csv
        - add tests, different delimiters, different comma/decimal characters
    10.c: Update CLI tools csv-to-bcsv and bcsv2csv to use new CSV Reader/Writer
    10.b: Update Benchmarks to use new CSV Reader/Writer
        

13. Update Tools and Libraries to use new API
    update and test CLI tool to use new C_API
    Update CLI tools to auto-detect FileFlags::ZERO_ORDER_HOLD and switch to ReaderZoH (or surface a clear message). 
    Medium: CLI tools don’t support ZoH outputs by default, while csv2bcsv still defaults to ZoH. The CLI tools (bcsvHeader, bcsvHead, bcsvTail, bcsv2csv) construct Reader<Layout> with Tracking disabled, which will refuse ZoH files. This is a UX/documentation mismatch: the default csv2bcsv output is not readable by those tools without --no-zoh. See csv2bcsv default in csv2bcsv.cpp:31-40 and the default reader usage in
    update and test python libraries
        - add python benchmarks
    update and test C# (Unity)
        - add C# benchmarks
        - add Unity / bcsv benchmark

Sonntag API freez
Sonntag: Switch Gears Dr
14. Implement new row encoding schema, using variable length encoding for float and integer values
    Main usage of bcsv is the handling of time-series data. Thus we expect many rcurring or constantly falling or raising values and values that are similar to previouse values. In many cases the full extend of a variable type is not required to encode the actual value.
    This feature is working on a per row level, not only a single cell/value as the current vle_encode / vle_decode functions do. It tries to provide a better balance between computational effort and compression and takes advantange on previouse data (time-series).
    
    Design goals:
    1. shortest possible encoding for: 
        - const values (Zero-Order-Hold), row(n).col(i) = row(n-1).col(i)
        - monoton raising/falling values (first-order), row(n).col(i) = row(n-1).col(i) + (row(n-1).col(i) - row(n-2).col(i))
        - bool to be stored directly as single bit
    2. small overhead in worst case scenarios
    3. exploit variable length to shorten storage for large types
    4. exploit delta encoding to shorten storage for large types
    5. allow easy comparision between two rows to identify equality in typical scenarios (no change, linear change)

    Not finaly decided yet:
        - store column encoding data in a dedictate area (row header) or close to data? Current pitch: lets implement a row header that does not require to follow byte alignment rules 
            - Bit-0 x N
            - Bis 1:2 x N (-bools, -rep)
            
        - 8bit aligned / byte aligned data?
        
    Bit-0 Repetion (Rep)
        0: check bits 1:5 for additional information on column encoding
        1: column encoding is same as previouse row bit3 1:5 are skipped
        Note: 
            1. For column type=bool, this column directly holds the value, no further bits required!
            2. For column hint=volatile, this column directly switches const/plain.
    Bit 1:2: column encoding
        00: const, previouse value is kept constant, bits 3:5 are skipped
        01: plain, bits 3:5 contain length of field, bits 6:14 contain data, data to simply contain the value we search
        10: extra, extrapolate using diff between row(n) = row(n-1) + row(n-1) - row(n-2), bits3:5 (length) and bits 6:14 (data) are skipped
        11: delta, delta relative to previouse row row(n) = row(n-1) + delta;
        Note: This filed is not present for variables maked volatile!
    Bit 3:5 length of the following data field, only present if data is provided and length is undefined, length is defined as number of bytes!
        for 1 byte types this field has a length of 0 bits (as data filed implictily is defined to length of 1 byte)
        for 2 byte types this field has a length of 1 bit  (0:short (1byte), 1:long (2bytes))
        for 4 byte types the filed has a maximum length of 2 bits (00:1byte, 01:2bytes, 10:3bytes, 11:4bytes)
        for 8 byte types the filed has a maximum length of 3 bits (double, unit64) it has a length of 
    Bit 6:14 data actual data stored for this variable

    Special cases: For small types (1 byte) avoid unnecessary overhead (complexity, runtime, control bits) that won't pay off

14. dictionary encoding for strings
    For strings a low cardinality is expected (many repetitions of the same string) are expected. Thus a dictionary based compression should be implemented. Exploit similar approach as for numerical values.
    Store string at its first occurens and assign ID (simply increment number, within the current packet)
    Other occurences just to reference that number. Run dedicated dicts per row, to balance speed and compression ratio. Block wide compression may come at a later point.

16. Compare against (document and benchmarks):
        parquet
        csv
        arrow
        influxDb
        excel

17. Write paper and release/submit

Parking LOT:
18. Create a concept to maintain backward compatibility at least for reading/translating files!
19. Implement File Flags (raw, stats)
    File Flags to provide information on file layout to switch between different Read / Write approaches. Enables different behaviour based on usecase and platform. 
    Flags to consider: 
        raw:        omit all the complex variable length encoding to save compute --> standard flat encoding schema
        stream:     omit packet structue and fileindex. also to save compute on constraint architecture, expected to come with significant performance constrains for random read. -> convert to Normal (packet bases) file structure in that case.
 [park] statistics: packet header to contain stasticical values on data included in each column, !min!, !max!, mean, median, std

20. Develop Direct Access Reader / Sparse Reader
        1.[sparse rows, dense columns]: Read fully populated row at specific predfined row indices, includes reading sequentialy within a row segment (row_start:row_end)
        2.[sparse rows, sparse columns]: Read a few columns per row, at specific predefined row indices, includes reading sequentialy within a row segment (row_start:row_end)
        3.[dense rows, spares columns]: Read a few columns per row, going over the entire file.
    
        Some thoughts:
            - use RowView to avoid deserilizing all columns if not needed
            - consider constraining string dictionary encoding to individual columns (lets do that as default, make blockwide (multi column) dictionary compression a future feature)
            - use Template parameters to "punch out" inividual columns

21. Create sampler function / API to select data based on certain conditions: 
    - Support C++ interface (iterate over individual rows)
    - Support Python interface (iterate over individual rows and block reads (pandas/numpy))

22. CLI tools:
    - Modify CLI tools to utilize random read functionality to avoid parsing the entire file (when possible):
        - bcsvTail
        - bcsvHead
        - bcsv2csv in combiniation with slicing
    - create: bcsv2Parquet
    - create: parquet2bcsv
    - create: bcsvInspect   (checks if the file is complet or damaged (i.e. index missing), can repair file/index, prints information on the file content)
    - create: bcsvSampler   (creates an down sampled version, using specific rules provided)
    - create: bcsvMore
    - create: bcsvCat
    - create: bcsvSed
    - create: bcsvCompress  (insepects the file and tries to apply a more agressive compression, using 2-phase inspection and a file wide dictionary)
    - create: bcsvIndex     (tool to rebuild index)
    - create: bcsvConvert   (converts from one bcsv file format to another, use to rebuild index or improve compression)

23. Make BCSV eco system to use piping features CLI, compareable to cat, more, sed

23. Implement Column Modifiers
    provides hints on what encoding schema to be used for this particular column
    modifiers to consider:
        index:      unique numbers
        volatile:   numbers are scattered, do not apply delta encoding
        monotonic:  apply higher order delta encoding as a "nearly" monotonic (smooth) behaviour is expected
        ordered:    data to fall or grow in ordered fashion 


LOW PRIORITY / Known Issues (from 8.h review, 2026-02-12):
    - hasAnyChanges() conflates bool values with change flags. Semantically correct for ZoH wire protocol
      but name is misleading. Consider renaming to hasDataToSerialize() or adding a comment for callers.
    - serializeToZoH writes raw bits_.data() via memcpy — architecture-dependent byte order. 
      Not portable across different endianness or word sizes. Document in wire format spec if cross-platform needed.
    - Bool changed flag in mutable visit() is passed to the visitor but silently ignored on write-back. 
      Document that changed parameter is read-only for bool columns.
    - setColumns(vector<string>, vector<ColumnType>) does not trigger observer notification (pre-existing). 
      Rows attached to layout will be out-of-sync if this overload is called. Add notification or document restriction.