Decisions: The file format uses 8-bit/1-byte alignment. Higher-order alignment is not considered, as we favor small file size. For temporary data and algorithms, higher-order alignment should be used.

[x] 0.  Write project mission statement, upload to git hub
    Aims on combining the easy usage of CSV files with the performance and file size of a binary format. It should perform specifically well for large time series data sets and also cover small embedded platforms. Therefore it is an explicit requirement that we support files that are larger than the available memory (RAM), thus the file is read in chunks or rows. For the same reason the computational effort should be reasonable small. In order to support real-time processing the call time to write or read data should be constant. 
    The file should exploit time-series data structure to reduce file size. Specifically it should be efficient on recording  Binary Wave Forms and constant data (spars recording / events) to achieve good compression. Compression ratio to be balanced with requirements on computational and streaming. It should be possible to read data from the file even if write was interrupted. We want to be able to retrieve the last fully written row. The file format is to be optimized for sequential row wise read and write operations, but should support random read with acceptable speeds, too. 
    The file should feel natural to programmers and compete with CSV with respect of ease of usage. Therefore, we avoid definition or validation of a schema. The file should be defined from within the programming language itself (C/C++, python, C# or …), without the need to run another tool or configuration file format. This is different to FlatBuffers or ProtoBuff. The file structure itself is documented in the file, similar to a header in a CSV. The header in BCSV is mandatory, as we won’t waste storage for delimiters. BCSV also enforces the type defined in the header for each column on every row. Thus you only need to validate the file structure once when you open a file and can rely on every row to show up with a valid type in each column (cell). 

    Design Goals:
    -	Sequential recording of a 1000 channel stream with 1KHz on an STM32F4 and 10KHz on a STM32F7, Zynq7000, RaspPi-nano
    -	Small file growth < 1KB/s for 1000 channel stream with 10Khz if nothing happens, except counting a clock/counter. 
    -	Processing >= 1 million rows/sec for a 1000 channel stream on a modern Zen3 CPU core.
    -	<30% file size compared to CSV comparing a 1000 channel stream
    -	Compatible with C/C++, C# and Python

    A set if CLI tools should help working with the files, supporting common work flows (view files, filter files, split and merge files, convert from and other formats).
    Who is the customer: Anybody how is running metrology tasks with digital tools. 


[x] 0. Test and Stabilize direct access feature
    - build and run unit tests  (passed)
    - build and run benchmarks  (passed)
[x] 1. Test and Stabilize CLI tools:
    a. bcsv2csv
    b. csv2bcsv
    c. bcsvHead
    d. bcsvTail
[x] 2. Test and Stabilize C_API -> C#
[x] 3. Test and Stabilize Python

[x] 4. Refactor VLE (Variabel Length Encoding):
    Note:
        We won't distinguish between VLE and BLE as we only use "block length encoding" to implement variable length encoding. This means we are string the number of additioaly required bytes required in an block of consecutive bits at the very begining of the byte stream considered.
        Use templates and expolit knowlegde about the type to improve performance.
            - fallthrough for single byte types
            - zigzag encoding only for signed types
            - destinguish between truncated and normal length mode (truncated means the bits required to encode the length are taken from the actual value, e.g. for uint64_t truncated only leaves 61bit for value)
            - we encode the number of ADDITIONAL bytes required to encode/decode the value, additional to the 1st byte that must be present anyway e.g. for uint64_t the range is 0 to 7)
            - avoid loading or shifiting indivudal bytes and replace this with opertions on larger registers if possible to achive higher performance, consider speculative operations and masking (e.g. always load 8bytes for an uint64) if performance can be gained without corrupting the I/O. We now from experiments that this can be highly profitable, espacially when used on data already in memory, but that travesing back and forth in a std::stream has a significant performance penaltiy and must be avoided. 

    Functions required:
    - size_t vle_encode(const T &value, void* dst, size_t dst_capacity)  //returns the number of bytes written to dst to store the value
    - void   vle_encode(const T &value, ByteBuffer &bufferToAppend)      //appends the bytes to the ByteBuffer increasing its size
    - size_t vle_decode(T &value, void* src, size_t src_capacity) //returns the number of bytes consumed
    - T      vle_decode(std::span<std::byte> &bufferToRead)     //span gets updated, removing the bytes consumed
    - bool   vle_read(T &value, &istream, *hash = 0)
    - bool   vle_write(const T &value, &ostream, *hash = 0)

   
6. Modify Row and Layout to not use std::variants, consider alignment
    get it compiled
    pass tests
    pass benchmark
    update tools
    commit

    actions: overload std::array with own array that performes bound checking in debug builds!
    
7. Improve benchmarks (more realistic use cases, better coverage, better insights)
    - add csv-to-bcsv to large_scale_benchmark
    - add bcsv-to-csv to large_scale_benchmark
    - add performance metrics to bcsv2csv and csv2bcsv tools
    - port benchmarks back to run old version (perviouse week) and compare

8. Add visit() function to Row, RowStatic, RowView, RowViewStatic enable fast operations on the data 
   Add support for dupplicated names, to improve flexibility of use
   Refactor bitset_dynamic to use full register width instead of narrow bytes. Thus we want to use 64bit types on 64bit platform. Consider alignment.

9. Consider a robust handling of ColumnNames. 
    - do not modify ColumnNames protected by ""
    - allow dupplicate ColumnNames
10. Implement File Flags (raw, stats)
    File Flags to provide information on file layout to swtich between different Read / Write approaches. Enables different behaviour based on usecase and platform. 
    Flags to consider: 
        raw:        omit all the complex variable length encoding to save compute --> standard flat encoding schema
        stream:     omit packet structue and fileindex. also to save compute on constraint architecture, expected to come with significant performance constrains for random read. -> convert to Normal (packet bases) file structure in that case.
        statistics: packet header to contain stasticical values on data included in each column, !min!, !max!, mean, median, std
11. Implement Column Modifiers
    provides hints on what encoding schema to be used for this particular column
    modifiers to consider:
        index:      unique numbers
        volatile:   numbers are scattered, do not apply delta encoding
        monotonic:  apply higher order delta encoding as a "nearly" monotonic (smooth) behaviour is expected
        ordered:    data to fall or grow in ordered fashion 

12. Implement new row encoding schema, using variable length encoding for float and integer values
    Main usage of bcsv is the handling of time-series data. Thus we expect many rcurring or constantly falling or raising values and values that are similar to previouse values. In many cases the full extend of a variable type is not required to encode the actual value.
    This feature is working on a per row level, not only a single cell/value as the current vle_encode / vle_decode functions do. It tries to provide a better balance between computational effort and compression and takes advantange on previouse data (time-series).
    
    Design goals:
    1. shortest possible encoding for: 
        - const values (Zero-Order-Hold), row(n).col(i) = row(n-1).col(i)
        - monoton raising/falling values (first-order), row(n).col(i) = row(n-1).col(i) + (row(n-1).col(i) - row(n-2).col(i))
        - bool to be stored directly as single bit
    2. small overhead in worst case scenarios
    3. exploit variable length to shorten storage for large types
    4. exploit delta encoding to shorten storage for large types
    5. allow easy comparision between two rows to identify equality in typical scenarios (no change, linear change)

    Not finaly decided yet:
        - store column encoding data in a dedictate area (row header) or close to data? Current pitch: lets implement a row header that does not require to follow byte alignment rules 
            - Bit-0 x N
            - Bis 1:2 x N (-bools, -rep)
            
        - 8bit aligned / byte aligned data?
        
    Bit-0 Repetion (Rep)
        0: check bits 1:5 for additional information on column encoding
        1: column encoding is same as previouse row bit3 1:5 are skipped
        Note: 
            1. For column type=bool, this column directly holds the value, no further bits required!
            2. For column hint=volatile, this column directly switches const/plain.
    Bit 1:2: column encoding
        00: const, previouse value is kept constant, bits 3:5 are skipped
        01: plain, bits 3:5 contain length of field, bits 6:14 contain data, data to simply contain the value we search
        10: extra, extrapolate using diff between row(n) = row(n-1) + row(n-1) - row(n-2), bits3:5 (length) and bits 6:14 (data) are skipped
        11: delta, delta relative to previouse row row(n) = row(n-1) + delta;
        Note: This filed is not present for variables maked volatile!
    Bit 3:5 length of the following data field, only present if data is provided and length is undefined, length is defined as number of bytes!
        for 1 byte types this field has a length of 0 bits (as data filed implictily is defined to length of 1 byte)
        for 2 byte types this field has a length of 1 bit  (0:short (1byte), 1:long (2bytes))
        for 4 byte types the filed has a maximum length of 2 bits (00:1byte, 01:2bytes, 10:3bytes, 11:4bytes)
        for 8 byte types the filed has a maximum length of 3 bits (double, unit64) it has a length of 
    Bit 6:14 data actual data stored for this variable

    Special cases: For small types (1 byte) avoid unnecessary overhead (complexity, runtime, control bits) that won't pay off
13. dictionary encoding for strings
    For strings a low cardinality is expected (many repetitions of the same string) are expected. Thus a dictionary based compression should be implemented. Exploit similar approach as for numerical values.
    Store string at its first occurens and assign ID (simply increment number, within the current packet)
    Other occurences just to reference that number

14. Benchmark and improve random access / random read methods:
    Row row(size_t index) {
        if(index == current_index+1) {
            //fall back to sequential
            readNext();
            return row_;
        }
        //check if row is in current packet, if not read packet required, build an file wide index if needed
        // use specific heuristics for ZoH, i.e. traverse several rows back and for to avoid complex reads
    }
    const auto& cell(size_t rowIndex, size_t colIndex) // To directly access a cell, use RowView as a lightweight abstraction to avoid full deserialization

15. Create sampler function / API to select data based on certain conditions: 
    - Support C++ interface (iterate over individual rows)
    - Support Python interface (iterate over individual rows and block reads (pandas/numpy))

16. CLI tools:
    - Modify CLI tools to utilize random read functionality to avoid parsing the entire file (when possible):
        - bcsvTail
        - bcsvHead
        - bcsv2csv in combiniation with slicing
    - create: bcsv2Parquet
    - create: parquet2bcsv
    - create: bcsvInspect   (checks if the file is complet or damaged (i.e. index missing), can repair file/index, prints information on the file content)
    - create: bcsvSampler   (creates an down sampled version, using specific rules provided)
    - create: bcsvCompress  (insepects the file and tries to apply a more agressive compression, using 2-phase inspection and a file wide dictionary)
    - create: bcsvIndex     (tool to rebuild index)
    - create: bcsvConvert   (converts from one bcsv file format to another, use to rebuild index or improve compression)

17. Create a concept to maintain backward compatibility at least for reading/translating files!
18. Compare against (document and benchmarks):
        parquet
        csv
        arrow
        influxDb
        excel
19. Write paper and release/submit



